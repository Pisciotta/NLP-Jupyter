{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit) is a Python platform for human language data processing.\n",
    "<p>A complete list of NLTK modules can be found here: https://www.nltk.org/py-modindex.html</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Multilingual Wordnet (OMW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>While a <b>dictionary</b> is used to define words, a <b>thesaurus</b> groups words with similar meaning. <b>WordNet</b> is a lexical database for the English combining dictionary and thesaurus functionalities.</p>\n",
    "<p>The goal of <b>Open Multilingual Wordnet</b> is to make it easy to use wordnets in multiple languages.</p>\n",
    "<p>To use Open Multilingual WordNet you need to download it (Corpora -> 'owm') interactively using a GUI interface of the <i>NLTK Downloader</i> from Python terminal:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Note: if you only download wordnet instead of owm, there will be no possibility to work with other languages</i>\n",
    "For the following examples you also need to download and import further corpora:\n",
    "- webtext (containing files in txt format)\n",
    "- stopwords (to filter out stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import webtext\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each corpus has different files containing some text. To get a list of such files of e.g. the above imported wordnet (as wn) corpus, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cntlist.rev', 'lexnames', 'index.sense', 'index.adj', 'index.adv', 'index.noun', 'index.verb', 'data.adj', 'data.adv', 'data.noun', 'data.verb', 'adj.exc', 'adv.exc', 'noun.exc', 'verb.exc')\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the list of words inside a corpus we use the .words() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cookie', 'Manager', ':', '\"', 'Don', \"'\", 't', ...]\n"
     ]
    }
   ],
   "source": [
    "print(webtext.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordnet (OMW) => Synset basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Synset</b> are wordnet instances grouping synonymous words that express the same concept.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('ghost.n.01'), Synset('figment.n.01'), Synset('apparition.n.01')]\n"
     ]
    }
   ],
   "source": [
    "syn = wordnet.synsets('fantasma', lang='ita')\n",
    "print(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:  ghost.n.01\n",
      "DEFINITION:  a mental representation of some haunting experience\n",
      "EXAMPLES:  ['he looked like he had seen a ghost', 'it aroused specters from his past']\n"
     ]
    }
   ],
   "source": [
    "print(\"NAME: \",syn[0].name())\n",
    "print(\"DEFINITION: \",syn[0].definition())\n",
    "print(\"EXAMPLES: \",syn[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('ghost.n.01.fantasma'), Lemma('ghost.n.01.ombra'), Lemma('ghost.n.01.spettro'), Lemma('ghost.n.01.spirito')]\n",
      "[Synset('apparition.n.03')]\n"
     ]
    }
   ],
   "source": [
    "print(syn[0].lemmas(lang='ita'))\n",
    "print(syn[0].hypernyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nltk.collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Collocations</b> are terms that can co-occur inside a sentence because of some kind of relation given by the common use of the language.\n",
    "\n",
    "A <b>bigram</b> is a sequence of two adjacent elements. What we are going to achieve with the following code is to list bigrams according to their collocation probability ranking / order within a given corpus (in our case webtext). So, <b>after stopwords filtering</b>, bigrams that co-occur more often than others are nearer to the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jack', 'sparrow'),\n",
       " ('teen', 'girl'),\n",
       " ('new', 'york'),\n",
       " ('teen', 'boy'),\n",
       " ('download', 'manager'),\n",
       " ('elizabeth', 'swann'),\n",
       " ('http', '://'),\n",
       " ('top', '***'),\n",
       " ('new', 'tab'),\n",
       " ('context', 'menu'),\n",
       " ('address', 'bar'),\n",
       " ('print', 'preview'),\n",
       " ('davy', 'jones'),\n",
       " ('little', 'boy'),\n",
       " ('mozilla', 'firebird'),\n",
       " ('bookmarks', 'toolbar'),\n",
       " ('little', 'girl'),\n",
       " ('location', 'bar'),\n",
       " ('flying', 'dutchman'),\n",
       " ('old', 'man')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "\n",
    "# Load all words of webtext corpus\n",
    "words = [w.lower() for w in webtext.words()] \n",
    "\n",
    "# Creating a BigramCollocationFinder object for later use\n",
    "# from the previously created corpus words list\n",
    "bigramColloc = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# Filtering stopwords out from the BigramCollocationFinder object\n",
    "bigramColloc.apply_word_filter(filter_stops)\n",
    "\n",
    "# Get the first 20 results of two-word-sequences in our (already filtered)\n",
    "# corpus according to the likelihood of both words to be found together\n",
    "# in the shown sequence\n",
    "bigramColloc.nbest(BigramAssocMeasures.likelihood_ratio, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teen', 'girl'),\n",
       " ('jack', 'sparrow'),\n",
       " ('teen', 'boy'),\n",
       " ('new', 'tab'),\n",
       " ('download', 'manager'),\n",
       " ('new', 'york'),\n",
       " ('little', 'girl'),\n",
       " ('top', '***'),\n",
       " ('little', 'boy'),\n",
       " ('address', 'bar'),\n",
       " ('context', 'menu'),\n",
       " ('bookmarks', 'toolbar'),\n",
       " ('old', 'man'),\n",
       " ('elizabeth', 'swann'),\n",
       " ('new', 'window'),\n",
       " ('http', '://'),\n",
       " ('mozilla', 'firebird'),\n",
       " ('drunk', 'guy'),\n",
       " ('location', 'bar'),\n",
       " ('old', 'lady')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, instead, bigrams are ordered according to their frequency\n",
    "# in the corpus\n",
    "bigramColloc.nbest(BigramAssocMeasures.raw_freq, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
